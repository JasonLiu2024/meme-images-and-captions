{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10e437710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import importlib as imp\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 8, 4, 9, 1, 6, 7, 3, 0, 5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.asarray(list(range(10)))\n",
    "np.random.shuffle(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: METAL\n",
      "Device: METAL\n"
     ]
    }
   ],
   "source": [
    "import data\n",
    "imp.reload(data)\n",
    "manager = data.Meme_n_Caption_Data_Manager(\n",
    "    FILENAME_all_image_embeddings='image_embeddings.pt',\n",
    "    FILENAME_all_caption_embeddings='caption_embeddings.pt',\n",
    "    FILENAME_MAP_image_index_TO_caption_indices='image_index_TO_caption_index.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = list(range(len(manager.MAP_image_index_TO_caption_indices[0])))\n",
    "np.random.shuffle(all_indices)\n",
    "train_ratio, validate_ratio, test_ratio = 0.7, 0.1, 0.2\n",
    "assert train_ratio + validate_ratio + test_ratio == 1, \"STOP! Train/Validate/Test split NO add to 1\"\n",
    "test_indices = np.asarray(all_indices[int(len(all_indices) * (1 - test_ratio)):])\n",
    "train_and_validate_indices = np.asarray(all_indices[:int(len(all_indices) * (1 - test_ratio))])\n",
    "assert len(train_and_validate_indices) + len(test_indices) == len(all_indices), \"STOP! T/V/T indices split wrong\"\n",
    "# train_indices = all_indices[:int(len(all_indices) * (train_ratio))]\n",
    "# validate_indices = all_indices[int(len(all_indices) * (train_ratio)):int(len(all_indices) * (1 - test_ratio))]\n",
    "# assert len(train_indices) + len(validate_indices) + len(test_indices) == len(all_indices), \"STOP! T/V/T indices split wrong\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920 480\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "DOER_CV = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "indices = np.arange(13)\n",
    "i = None\n",
    "for t1, t2 in DOER_CV.split(np.asarray(train_and_validate_indices)[:, None], train_and_validate_indices):\n",
    "    print(len(t1), len(t2))\n",
    "    i = t1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_and_validate_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = manager.get_dataloader(batch_size=10, RATIO_negative_samples_over_positive_samples=1, INDICES_usable_captions=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2048]) torch.Size([10, 384]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for b in dl:\n",
    "    a, b, c = b\n",
    "    print(a.shape, b.shape, c.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: METAL\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Invalid buffer size: 8.79 GB",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m m\n\u001b[1;32m     36\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1e-3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m64\u001b[39m, \n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_type\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mratio_-_samples_count_over_+_samples_count\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.0\u001b[39m}\n\u001b[0;32m---> 38\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSiamese\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 12\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(m, hyperparameters, plot)\u001b[0m\n\u001b[1;32m      7\u001b[0m DOER_CV \u001b[38;5;241m=\u001b[39m KFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_indicesCEPTION, validate_indicesCEPTION \u001b[38;5;129;01min\u001b[39;00m DOER_CV\u001b[38;5;241m.\u001b[39msplit(\n\u001b[1;32m      9\u001b[0m     np\u001b[38;5;241m.\u001b[39masarray(train_and_validate_indices)[:, \u001b[38;5;28;01mNone\u001b[39;00m], train_and_validate_indices):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     train_losses \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain(model\u001b[38;5;241m=\u001b[39mm, \n\u001b[0;32m---> 12\u001b[0m         dataloader_train\u001b[38;5;241m=\u001b[39m\u001b[43mmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mRATIO_negative_samples_over_positive_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mratio_-_samples_count_over_+_samples_count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mINDICES_usable_captions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_and_validate_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_indicesCEPTION\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, hyperparameters\u001b[38;5;241m=\u001b[39mhyperparameters)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m plot:\n\u001b[1;32m     17\u001b[0m         f, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n",
      "File \u001b[0;32m~/Desktop/561/data.py:35\u001b[0m, in \u001b[0;36mMeme_n_Caption_Data_Manager.get_dataloader\u001b[0;34m(self, batch_size, RATIO_negative_samples_over_positive_samples, INDICES_usable_captions)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size : \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     32\u001b[0m         RATIO_negative_samples_over_positive_samples : \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m     33\u001b[0m         INDICES_usable_captions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m dataloader\u001b[38;5;241m.\u001b[39mDataLoader:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataloader\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m---> 35\u001b[0m         dataset\u001b[38;5;241m=\u001b[39m\u001b[43mMeme_n_Caption_Dataset_Standard\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m            \u001b[49m\u001b[43mRATIO_negative_samples_over_positive_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m            \u001b[49m\u001b[43mINDICES_usable_captions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMAP_image_index_TO_caption_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_image_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_caption_embeddings\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[1;32m     40\u001b[0m         drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,)\n",
      "File \u001b[0;32m~/Desktop/561/data.py:87\u001b[0m, in \u001b[0;36mMeme_n_Caption_Dataset_Standard.__init__\u001b[0;34m(self, RATIO_negative_samples_over_positive_samples, INDICES_usable_captions, MAP_image_index_TO_caption_indices, all_image_embeddings, all_caption_embeddings)\u001b[0m\n\u001b[1;32m     84\u001b[0m     caption_embeddings\u001b[38;5;241m.\u001b[39mappend(all_caption_embeddings[\n\u001b[1;32m     85\u001b[0m         MAP_image_index_TO_caption_indices[image_index_of_a_different_image][INDICES_usable_captions][caption_index]])\n\u001b[1;32m     86\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat))\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaption_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(caption_embeddings)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(labels)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Invalid buffer size: 8.79 GB"
     ]
    }
   ],
   "source": [
    "from numpy.typing import NDArray\n",
    "import matplotlib.pyplot as plt\n",
    "import model\n",
    "imp.reload(model)\n",
    "def cross_validate(m: nn.Module, hyperparameters,\n",
    "        plot: bool=False):\n",
    "    DOER_CV = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    for train_indicesCEPTION, validate_indicesCEPTION in DOER_CV.split(\n",
    "        np.asarray(train_and_validate_indices)[:, None], train_and_validate_indices):\n",
    "        # training\n",
    "        train_losses = model.train(model=m, \n",
    "            dataloader_train=manager.get_dataloader(\n",
    "                batch_size=hyperparameters['batch_size'], \n",
    "                RATIO_negative_samples_over_positive_samples=hyperparameters['ratio_-_samples_count_over_+_samples_count'], \n",
    "                INDICES_usable_captions=train_and_validate_indices[train_indicesCEPTION]), hyperparameters=hyperparameters)\n",
    "        if plot:\n",
    "            f, ax = plt.subplots(figsize=(4, 4))\n",
    "            ax.plot(train_losses)\n",
    "        break\n",
    "    return m\n",
    "    # inference\n",
    "    test_losses, test_accuracy, aurocs, auprcs = model.test(model=m, dataloader_test=data.get_dataloader(\n",
    "        hyperparameters=hyperparameters,\n",
    "        selected_features=features_test,\n",
    "        selected_labels=labels_test))\n",
    "    auroc_mean, auroc_std = np.asarray(aurocs).mean(), np.asarray(aurocs).std()\n",
    "    auprc_mean, auprc_std = np.asarray(auprcs).mean(), np.asarray(auprcs).std()\n",
    "    losses_mean, losses_std = np.asarray(test_losses).mean(), np.asarray(test_losses).std()\n",
    "    # print(m)\n",
    "    print(f\"Test AUROC: {auroc_mean, auroc_std}\")\n",
    "    print(f\"Test AUPRC: {auprc_mean, auprc_std}\")\n",
    "    print(f\"Test Loss: {losses_mean, losses_std}\")\n",
    "    print(f\"Test Accuracy: %{test_accuracy}\")\n",
    "    return m\n",
    "\n",
    "hyperparameters = {'learning_rate': 1e-3, 'batch_size': 64, \n",
    "    'optimizer_type': torch.optim.Adam, 'ratio_-_samples_count_over_+_samples_count': 1.0}\n",
    "m = cross_validate(model.Siamese(), hyperparameters,\n",
    "    plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
